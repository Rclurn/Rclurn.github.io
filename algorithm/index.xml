<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm on Rclurn</title>
    <link>https://Rclurn.github.io/algorithm/</link>
    <description>Recent content in Algorithm on Rclurn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>liu-huo@live.com (æ‘¸é“æ ¡å°‰ğŸ¥‰)</managingEditor>
    <webMaster>liu-huo@live.com (æ‘¸é“æ ¡å°‰ğŸ¥‰)</webMaster>
    <copyright>Â© 2021 &lt;a href=&#34;https://github.com/miiiku/hugo-theme-kagome&#34;&gt;kagome&lt;/a&gt;.</copyright>
    <lastBuildDate>Wed, 25 Sep 2024 13:23:41 +0800</lastBuildDate><atom:link href="https://Rclurn.github.io/algorithm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Transformer</title>
      <link>https://Rclurn.github.io/algorithm/transformer/</link>
      <pubDate>Wed, 25 Sep 2024 13:23:41 +0800</pubDate>
      <author>liu-huo@live.com (æ‘¸é“æ ¡å°‰ğŸ¥‰)</author>
      <guid>https://Rclurn.github.io/algorithm/transformer/</guid>
      <description>Tokenizer Embedding ä¸‰è§’ä½ç½®ç¼–ç  class PositionEmbedding(nn.Module): def __init__(self, dim, base=10000, max_seq_len=512): super().__init__() self.dim = dim pe = torch.zeros(max_seq_len, dim) for pos in range(max_seq_len): for i in range(0, dim, 2): pe[pos][i] = math.sin(pos / (base ** (2 * i / self.dim))) pe[pos][i + 1] = math.cos(pos / (base ** (2 * (i + 1) / self.dim))) pe = pe.unsqueeze(dim=0) self.register_buffer(&amp;#39;pe&amp;#39;, pe) def forward(self, x): x = x</description>
    </item>
    
    <item>
      <title>å…¬å¼ç†è§£</title>
      <link>https://Rclurn.github.io/algorithm/%E5%85%AC%E5%BC%8F%E7%90%86%E8%A7%A3/</link>
      <pubDate>Mon, 23 Sep 2024 10:55:00 +0800</pubDate>
      <author>liu-huo@live.com (æ‘¸é“æ ¡å°‰ğŸ¥‰)</author>
      <guid>https://Rclurn.github.io/algorithm/%E5%85%AC%E5%BC%8F%E7%90%86%E8%A7%A3/</guid>
      <description>Softmax softmaxæ˜¯ä¸€ä¸ªå¸¸ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ä¸­çš„æ¿€æ´»å‡½æ•°ï¼Œå…¶æ ¸å¿ƒä¸ºè§„èŒƒåŒ–æ¦‚ç‡è¾“å‡ºï¼Œå¹¶ä¿è¯æ€»å’Œä¸º1ï¼ŒåŒæ—¶èƒ½å¤Ÿè®©æ¨¡å‹ä¿æŒå¯å¯¼ï¼Œä½œä¸ºç¥ç»ç½‘ç»œè¾“å‡ºå±‚çš„ä¸€éƒ¨</description>
    </item>
    
    <item>
      <title>Parameter Efficient Fine Tuning</title>
      <link>https://Rclurn.github.io/algorithm/parameter-efficient-fine-tuning/</link>
      <pubDate>Sat, 21 Sep 2024 16:37:17 +0800</pubDate>
      <author>liu-huo@live.com (æ‘¸é“æ ¡å°‰ğŸ¥‰)</author>
      <guid>https://Rclurn.github.io/algorithm/parameter-efficient-fine-tuning/</guid>
      <description>å¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ æ€»è§ˆ Prefix Tuning ğŸš€ äººå·¥è®¾è®¡çš„promptæ¨¡ç‰ˆå¯èƒ½å¹¶ä¸æ˜¯æœ€ä¼˜çš„ï¼Œè‡ªåŠ¨åŒ–æœç´¢æ¨¡ç‰ˆæˆæœ¬ä¹Ÿæ¯”è¾ƒé«˜ï¼› ä¼ ç»Ÿå¾®è°ƒèŒƒå¼å¯¹æ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦å­˜å‚¨ä¸€ä»½å¾®è°ƒ</description>
    </item>
    
    <item>
      <title>åˆ†è¯æ–¹å¼</title>
      <link>https://Rclurn.github.io/algorithm/%E5%88%86%E8%AF%8D%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Fri, 01 Mar 2024 11:09:56 +0800</pubDate>
      <author>liu-huo@live.com (æ‘¸é“æ ¡å°‰ğŸ¥‰)</author>
      <guid>https://Rclurn.github.io/algorithm/%E5%88%86%E8%AF%8D%E6%96%B9%E5%BC%8F/</guid>
      <description>BPEç®—æ³•çš„ç®€å•å®ç° from collections import defaultdict corpus = [ &amp;#34;This is the Hugging Face Course.&amp;#34;, &amp;#34;This chapter is about tokenization.&amp;#34;, &amp;#34;This section shows several tokenizer algorithms.&amp;#34;, &amp;#34;Hopefully, you will be able to understand how they are trained and generate tokens.&amp;#34; ] word_freqs = difaultdict(int) for text in corpus: for word in text.split(): word_freqs[word] += 1 vocab = [] for word in word_freqs.keys():</description>
    </item>
    
  </channel>
</rss>
