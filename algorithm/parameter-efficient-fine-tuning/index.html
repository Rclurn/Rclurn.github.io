<!DOCTYPE html>
<html><head>
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

   
    <meta name="author" content="摸铁校尉🥉">
  

   
    
    
    
    
    
    
    <meta name="keywords" content="Machine Learning,kagome">
  

   
    <meta name="description" content="记录生活点点点点点点点滴">
  

  <meta name="generator" content="Hugo 0.105.0">
  
  
  
  
  
  
  <title>Parameter Efficient Fine Tuning 🌟 Rclurn</title>

  <meta property="og:title" content="Parameter Efficient Fine Tuning" />
<meta property="og:description" content="记录生活点点点点点点点滴" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rclurn.github.io/algorithm/parameter-efficient-fine-tuning/" /><meta property="article:section" content="algorithm" />
<meta property="article:published_time" content="2024-09-21T16:37:17+08:00" />
<meta property="article:modified_time" content="2024-09-21T16:37:17+08:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Parameter Efficient Fine Tuning"/>
<meta name="twitter:description" content="记录生活点点点点点点点滴"/>


  
  
  
  
  <link rel="stylesheet" href="https://rclurn.github.io/assets/css/style.min.699f46622e3574a7f9ecca4c5877067845fe89ffcd9c4335df2dff23ddae215e.css" integrity="sha256-aZ9GYi41dKf57MpMWHcGeEX&#43;if/NnEM13y3/I92uIV4=">

  <script src="https://rclurn.github.io/assets/js/main.min.182da266209851bc7c828aa7377f98f914e1e76c8decdd53a6cbe9bffea92cde.js" integrity="sha256-GC2iZiCYUbx8goqnN3&#43;Y&#43;RTh52yN7N1Tpsvpv/6pLN4="></script>
  
  </head><body><header class="header-container layout-block layout-padding">
  <div class="header-inner content-padding-large soft-size--large soft-style--box">
    <div class="header-logo">
      <a href="https://rclurn.github.io/"><h1>Rclurn</h1></a>
    </div>
    <nav class="header-nav">
      <div class="header-nav--btn">
        <div class="btn-item"></div>
        <div class="btn-item"></div>
        <div class="btn-item"></div>
      </div>
      <div class="header-nav--list">
        <div>
          
          <a class="list-item soft-size--small soft-style--hover soft-style--active" href="/" title="">🏠Home</a>
          
          <a class="list-item soft-size--small soft-style--hover soft-style--active" href="/projects" title="">🤖Projects</a>
          
          <a class="list-item soft-size--small soft-style--hover soft-style--active" href="/paper" title="">📝Paper with code</a>
          
          <a class="list-item soft-size--small soft-style--hover soft-style--active" href="/algorithm" title="">👹Algorithm</a>
          
          <a class="list-item soft-size--small soft-style--hover soft-style--active" href="/about" title="">🤡About</a>
          
        </div>
      </div>
    </nav>
  </div>
</header><main id="content">
    

    <div class="single-container layout-block">
      <div class="article-info">
        <div class="article-header layout-padding">
          <div class="article-cover card-container content-padding-large soft-size--large soft-style--box img">

  <div class="card-cover">
    
      <img src="https://qiniu.sukoshi.xyz/src/images/68686407_p0.jpg" alt="https://qiniu.sukoshi.xyz/src/images/68686407_p0.jpg" />
    
  </div>

  <div class="card-text">
    <h1 class="card-text--title">Parameter Efficient Fine Tuning</h1>
    
      <p class="card-text--row">2024-09-21 16:37</p>
      
      
      
        <ul class="card-text--tag">
          
            <li><a href="https://rclurn.github.io/categories/development/">Development</a></li></ul>
      

      
      
    
  </div>

</div>
        </div>

        <div class="article-content">
          <div class="markdown-body content-padding-large soft-size--large soft-style--box">
            <h1 id="大模型高效微调技术">大模型高效微调技术</h1>
<h2 id="总览">总览</h2>
<p>
  <p><img src="https://s2.loli.net/2024/09/24/1vyXApnd2jH7xNz.png" alt="" loading="lazy" /></p>
</p>
<h2 id="prefix-tuning-httpsarxivorgpdf210100190">Prefix Tuning<a href="https://arxiv.org/pdf/2101.00190" target="_blank" rel="noopener"> 🚀</a></h2>
<ol>
<li>人工设计的prompt模版可能并不是最优的，自动化搜索模版成本也比较高；</li>
<li>传统微调范式对每个任务都需要存储一份微调后的权重，一方面微调耗时耗资源，另一方面也会占用很多存储空间；</li>
</ol>
<p>将预训练大模型固定，为特定的任务添加可训练的前缀，训练时通过更新前缀来找寻最优的软prompt，同时只需要保存不同的前缀就可以。针对decoder only架构，只需要在句子前添加前缀，通过合适的上文即可在固定的LLM的情况下去引导生成下文；针对encoder-decoder架构，分别在encoder、decoder的前面加入软prompt，encoder端是为了引导输入部分的编码，decoder端是为了引导后续token的生成。在细节上，为了防止直接更新Prefix的参数导致不稳定和性能下降的情况，通常在prefix后加入MLP结构，但是训练完成后只保留Prefix参数；其次，只在embedding层加入prefix的表现力不够，所以在每一层都加入了软prompt。</p>
<p>
  <p><img src="https://s2.loli.net/2024/09/21/GV8jf5PphtBnN1y.png" alt="" loading="lazy" /></p>
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PrefixEncoder</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        token_dim <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>token_dim
</span></span><span style="display:flex;"><span>        num_layers <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>num_layers
</span></span><span style="display:flex;"><span>        encoder_hidden_size <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>encoder_hidden_size
</span></span><span style="display:flex;"><span>        num_virtual_tokens <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>num_virtual_tokens
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embedding <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Embedding(num_virtual_tokens, token_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transform <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(token_dim, encoder_hidden_size),
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Tanh(),
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(encoder_hidden_size, num_layers <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> token_dim),
</span></span><span style="display:flex;"><span>        ) <span style="color:#75715e"># 这里最后为2 * token_dim 是因为直接将软prompt放入kv cache中了</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, prefix: torch<span style="color:#f92672">.</span>Tensor):
</span></span><span style="display:flex;"><span>        prefix_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(prefix)
</span></span><span style="display:flex;"><span>        past_key_values <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transform(prefix_tokens)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> past_key_values
</span></span></code></pre></div><h2 id="p-tuning对于prompt-tuning的改进-httpsarxivorgpdf210310385">P-Tuning(对于Prompt Tuning的改进)<a href="https://arxiv.org/pdf/2103.10385" target="_blank" rel="noopener"> 🚀</a></h2>
<p>相较于Prefix Tuning，P Tuning只在embedding层加入软prompt；同时插入的位置也不一定是开头，这里的出发点是把人工设计的模版中的真实token替换为可微的虚拟token，所以插入位置可以选择；虚拟token也不是直接通过nn.Embedding随机初始化的，理论上这些虚拟token应该是相关的，所以作者使用了一个LSTM+MLP的结构来编码这些虚拟token，可以使训练时收敛更快，效果更好。</p>
<p>
  <p><img src="https://s2.loli.net/2024/09/21/dqatPKBb6ezoiul.png" alt="" loading="lazy" /></p>
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PromptEncoder</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>token_dim <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>token_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>token_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>token_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_size <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>encoder_hidden_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>total_virtual_tokens <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>num_virtual_tokens <span style="color:#f92672">*</span> config<span style="color:#f92672">.</span>num_transformer_submodules
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>encoder_type <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>encoder_reparameterization_type
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embedding <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Embedding(self<span style="color:#f92672">.</span>total_virtual_tokens, self<span style="color:#f92672">.</span>token_dim)
</span></span><span style="display:flex;"><span>        lstm_dropout <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>encoder_dropout
</span></span><span style="display:flex;"><span>        num_layers <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>encoder_num_layers
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lstm_head <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>LSTM(
</span></span><span style="display:flex;"><span>            input_size<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>input_size,
</span></span><span style="display:flex;"><span>            hidden_size<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>hidden_size,
</span></span><span style="display:flex;"><span>            num_layers<span style="color:#f92672">=</span>num_layers,
</span></span><span style="display:flex;"><span>            dropout<span style="color:#f92672">=</span>lstm_dropout,
</span></span><span style="display:flex;"><span>            bidirectional<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>            batch_first<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, indices):
</span></span><span style="display:flex;"><span>        input_embeds <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(indices)
</span></span><span style="display:flex;"><span>        output_embeds <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mlp_head(self<span style="color:#f92672">.</span>lstm_head(input_embeds)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output_embeds
</span></span></code></pre></div><h2 id="p-tuning-v2对于prefix-tuning的改进-httpsarxivorgpdf211007602">P Tuning V2(对于Prefix Tuning的改进)<a href="https://arxiv.org/pdf/2110.07602" target="_blank" rel="noopener"> 🚀</a></h2>
<ol>
<li>去除了重参数化功能(Prefix中的MLP、P-Tuning中的LSTM)，因为提升很小，甚至没有提升</li>
<li>针对不同任务采用不同的长度</li>
<li>引入多任务学习，先在多任务的prompt上进行预训练，再适配下游任务</li>
<li>回归传统的分类标签范式，可以适配到序列标注任务上</li>
</ol>
<p>
  <p><img src="https://s2.loli.net/2024/09/21/WeqjaMxOTCoNp1l.png" alt="" loading="lazy" /></p>
</p>
<h2 id="adapter-tuning-httpsarxivorgpdf190200751">Adapter Tuning<a href="https://arxiv.org/pdf/1902.00751" target="_blank" rel="noopener"> 🚀</a></h2>
<p>Adapter结构主要由两个FFN组成，第一个FFN降维，第二个FFN升维，中间加入非线性变化，最后为了保证在训练之初Adapter初始化为0不会使模型有较大的偏差，做了一个残差连接的恒等映射。</p>
<p>
  <p><img src="https://s2.loli.net/2024/09/21/2T8sJuarq14fn69.png" alt="" loading="lazy" /></p>
</p>
<h2 id="lora-httpsarxivorgpdf210609685">LoRA<a href="https://arxiv.org/pdf/2106.09685" target="_blank" rel="noopener"> 🚀</a></h2>
<p>通过低质分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。具体而言，在原始模型的线性层的旁边增加一个旁路，通过两个矩阵A(in_features, r)、B(r, out_features)的点积来模拟原始参数的改变量，其中A采用高斯分布，B采用全0分布，这样在模型训练的一开始可以保证旁路的加入不会带来较大的偏差。LoRA微调相较于其他主要有以下几个优点：</p>
<ol>
<li>在推理上没有延迟，因为其可以在训练好后将旁路的权重直接加到原始权重上；</li>
<li>存储开销低，可以随意快速切换任务；</li>
<li>降低训练所需显存并加速训练（加速训练主要体现在多卡训练时，GPU之间的通讯量变少了）；</li>
<li>可以与现有的一些其他高效微调方法结合使用；</li>
</ol>
<p>
  <p><img src="https://s2.loli.net/2024/09/21/oE16KRvZxjbhUwk.png" alt="" loading="lazy" /></p>
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LoRALinear</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, base_layer, config):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>base_layer <span style="color:#f92672">=</span> copy<span style="color:#f92672">.</span>deepcopy(base_layer)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lora_A <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(base_layer<span style="color:#f92672">.</span>in_features, config<span style="color:#f92672">.</span>r)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lora_B <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(config<span style="color:#f92672">.</span>r, base_layer<span style="color:#f92672">.</span>out_features)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>base_layer<span style="color:#f92672">.</span>parameters():
</span></span><span style="display:flex;"><span>            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x_adj <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lora_B(self<span style="color:#f92672">.</span>lora_A(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>base_layer(x) <span style="color:#f92672">+</span> x_adj <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>alpha <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>r
</span></span></code></pre></div><h2 id="flash-attention-httpsarxivorgpdf220514135">Flash Attention<a href="https://arxiv.org/pdf/2205.14135" target="_blank" rel="noopener"> 🚀</a></h2>
<p>
  <p><img src="https://s2.loli.net/2024/09/23/7DrYbzLZF21SXm3.png" alt="" loading="lazy" /></p>
</p>
<p>在GPU的存储结构中，主要存在HBM(显存)和SRAM(计算时共享内存)，SRAM的读取速度远大于HBM，但是SRAM的存储容量却很小(每个只有100多kb)，一般无法直接存储大型矩阵，所以在进行大矩阵的运算时需要先将大矩阵分块然后读入SRAM中进行运算，然后将计算的结果在写入HBM。从上图可以看出，在Attention中，花费时间较多的是Dropout、Softmax、Mask等操作，这些操作属于存储访问密集型。所以，如果能减少对于HBM的访存，应该可以在一定程度上加速attention的计算过程。</p>
<p>在Flash Attention V1中主要解决的问题就是softmax如何分块进行，这里需要注意在实际运用中使用的是safe softmax，即在进行softmax的时候会将指数减去当前进行softmax序列的最大值，这样可以维持分子的范围在[0-1]之间，不会出现上溢；而分母至少为1，不会出现除0现象导致下溢。为了将attention融合为一个算子，需要对softmax这种需要一整行数据才能进行的操作进行改进，所以可以保存每个小块的最大值，然后根据数学公式来计算其最终的值，在理论上这种方式也是没有精度损失的。</p>
<p>在Flash Attention V2中，针对Causal Masking进行了优化，在attention计算中，如果某个分块全部被mask了，那么可以直接跳过这些分块而不必计算。</p>
<h2 id="deepspeed-httpsarxivorgpdf191002054">DeepSpeed<a href="https://arxiv.org/pdf/1910.02054" target="_blank" rel="noopener"> 🚀</a></h2>
<p>
  <p><img src="https://s2.loli.net/2024/09/23/ZqpjCo5sVa6K7vH.png" alt="" loading="lazy" /></p>
</p>
<h3 id="大模型训练内存开销的计算">大模型训练内存开销的计算</h3>
<p>大模型训练的显存占用，大模型参数量为#，以adam优化器和混合精度训练为例：</p>
<table>
<thead>
<tr>
<th style="text-align:center">模型参数</th>
<th style="text-align:center">模型梯度</th>
<th style="text-align:center">优化器</th>
<th style="text-align:center">激活值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2 * #（4 * #）</td>
<td style="text-align:center">2 * #</td>
<td style="text-align:center">3 * 4 * #</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<p>其中，激活值主要保存在前向传播中的一些中间结果；在混合精度训练中，模型的参数和模型的梯度都使用fp16/bf16，而优化器中保存的一阶动量、二阶动量均使用fp32进行存储，为了解决fp16/bf16的下溢问题(梯度是参数的一阶分量，往往很小，在乘上学习率后将更小，很容易在fp16/bf16上出现下溢的现象)，需要在优化器中保存一份对于模型权重的备份，计算权重的更新量时，利用这个备份权重，防止溢出现象。</p>
<h3 id="zero-stage-1">Zero Stage 1</h3>
<p>在模型训练的过程中，显存占用最大的是优化器，因此，Zero Stage 1的思路就是切分优化器状态，将优化器状态切分到不同的GPU上，每个GPU只保存优化器状态的一部分，这样可以极大的减小显存的占用(虽然会增加一定的通信量)，Zero Stage 1大约可以将显存减少为原始显存的四分之一。</p>
<h3 id="zero-stage-2">Zero Stage 2</h3>
<p>按照同样的理论，对优化器状态、模型梯度进行切分，大约可以将显存减少为原始显存的八分之一。</p>
<h3 id="zero-stage-3">Zero Stage 3</h3>
<p>对优化器状态、模型梯度、模型权重进行切分，大约可以将显存减少为原始显存的GPU数分之一。</p>

          </div>
        </div>
                 
      
  
  

  

  <div class="article-paging">
    
      <section class="post-paging--item card-container content-padding-primary soft-size--primary soft-style--box">
  <div class="card-cover" background-image-lazy data-img="https://qiniu.sukoshi.xyz/src/images/68686407_p0.jpg"></div>
  <div class="card-text">
    <a href="/algorithm/%E5%85%AC%E5%BC%8F%E7%90%86%E8%A7%A3/"><h4 class="card-text--title text-ellipsis">公式理解</h4></a>
    <p class="card-text--row">2024-09-23 10:55</p>
  </div>
</section>
    
    
      <section class="post-paging--item card-container content-padding-primary soft-size--primary soft-style--box">
  <div class="card-cover" background-image-lazy data-img="https://qiniu.sukoshi.xyz/src/images/68686407_p0.jpg"></div>
  <div class="card-text">
    <a href="/algorithm/%E5%88%86%E8%AF%8D%E6%96%B9%E5%BC%8F/"><h4 class="card-text--title text-ellipsis">分词方式</h4></a>
    <p class="card-text--row">2024-03-01 11:09</p>
  </div>
</section>
    
  </div>
</div>
  <aside class="widget-info">
    
<section class="aside-widget widget-author content-padding-large soft-size--large soft-style--box">
  <div class="widget-body">
    <div class="author-box avatar">
      
      <img class="author-avatar soft-size--round soft-style--box" src="https://s.gravatar.com/avatar/7b5a0b07a98895278cfa862b1f32ae8f?s=200&amp;r=g&amp;d=retro" alt="摸铁校尉🥉">
      
      <h2 class="author-name text-ellipsis">摸铁校尉🥉</h2>
      
      <p class="author-desc text-ellipsis">闭上眼睛，开始穿越!</p>
      
    </div>
  </div>
</section>


    

<section class="aside-widget widget-toc content-padding-large soft-size--large soft-style--box">
  <h2 class="widget-header">
    <div class="title">
      <span>Toc</span>
    </div>
  </h2>
  <div class="widget-body">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#总览">总览</a></li>
    <li><a href="#prefix-tuning-httpsarxivorgpdf210100190">Prefix Tuning<a href="https://arxiv.org/pdf/2101.00190"> 🚀</a></a></li>
    <li><a href="#p-tuning对于prompt-tuning的改进-httpsarxivorgpdf210310385">P-Tuning(对于Prompt Tuning的改进)<a href="https://arxiv.org/pdf/2103.10385"> 🚀</a></a></li>
    <li><a href="#p-tuning-v2对于prefix-tuning的改进-httpsarxivorgpdf211007602">P Tuning V2(对于Prefix Tuning的改进)<a href="https://arxiv.org/pdf/2110.07602"> 🚀</a></a></li>
    <li><a href="#adapter-tuning-httpsarxivorgpdf190200751">Adapter Tuning<a href="https://arxiv.org/pdf/1902.00751"> 🚀</a></a></li>
    <li><a href="#lora-httpsarxivorgpdf210609685">LoRA<a href="https://arxiv.org/pdf/2106.09685"> 🚀</a></a></li>
    <li><a href="#flash-attention-httpsarxivorgpdf220514135">Flash Attention<a href="https://arxiv.org/pdf/2205.14135"> 🚀</a></a></li>
    <li><a href="#deepspeed-httpsarxivorgpdf191002054">DeepSpeed<a href="https://arxiv.org/pdf/1910.02054"> 🚀</a></a>
      <ul>
        <li><a href="#大模型训练内存开销的计算">大模型训练内存开销的计算</a></li>
        <li><a href="#zero-stage-1">Zero Stage 1</a></li>
        <li><a href="#zero-stage-2">Zero Stage 2</a></li>
        <li><a href="#zero-stage-3">Zero Stage 3</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</section>


    






<section class="aside-widget widget-articles content-padding-large soft-size--large soft-style--box">
  <h2 class="widget-header">
    <div class="title">
      <span>Related Posts</span>
    </div>
  </h2>
  <div class="widget-body">
    <ul class="post-list">
      
      
        <li class="post-item"><a href="/algorithm/deepseek-v2/">DeepSeek V2</a></li>
      
        <li class="post-item"><a href="/algorithm/baichuan-2/">Baichuan 2</a></li>
      
        <li class="post-item"><a href="/algorithm/speculative-decoding/">Speculative Decoding</a></li>
      
        <li class="post-item"><a href="/algorithm/transformer/">Transformer</a></li>
      
        <li class="post-item"><a href="/algorithm/baai-general-embeddings/">BAAI General Embeddings</a></li>
      
        <li class="post-item"><a href="/algorithm/direct-preference-optimization/">Direct Preference Optimization</a></li>
      
    </ul>
  </div>
</section>


    




<section class="aside-widget widget-categories content-padding-large soft-size--large soft-style--box">
  <h2 class="widget-header">
    <div class="title">
      <span>Categories</span>
    </div>
  </h2>
  <div class="widget-body">
    <ul class="categories-list">
      
        <li>
          <a href="https://rclurn.github.io/categories/development/">Development</a>
          <span>11</span>
        </li>
      
        <li>
          <a href="https://rclurn.github.io/categories/%E5%B7%A5%E5%85%B7%E7%AE%B1/">工具箱</a>
          <span>3</span>
        </li>
      
        <li>
          <a href="https://rclurn.github.io/categories/%E4%B8%8A%E5%88%86%E6%9D%80%E5%99%A8/">上分杀器</a>
          <span>2</span>
        </li>
      
        <li>
          <a href="https://rclurn.github.io/categories/%E7%AB%9E%E8%B5%9B/">竞赛</a>
          <span>1</span>
        </li>
      
        <li>
          <a href="https://rclurn.github.io/categories/%E7%AE%97%E6%B3%95/">算法</a>
          <span>1</span>
        </li>
      
        <li>
          <a href="https://rclurn.github.io/categories/%E9%9D%A2%E8%AF%95/">面试</a>
          <span>1</span>
        </li>
      
    </ul>
  </div>

  
</section>


    




  </aside>
</div>
  </main><footer class="footer-container layout-block">
  
  <div class="social-icons">
    
      <a class="soft-size--primary soft-style--box" href="https://github.com/rnaker" target="_blank" rel="noopener noreferrer">
          
        <svg class="icon icon-github" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
          <path d="M64.6 512c0 195.6 125.4 361.9 300.1 422.9 23.5 5.9 19.9-10.8 19.9-22.2v-77.6c-135.8 15.9-141.3-74-150.5-89-18.5-31.5-61.9-39.5-49-54.5 31-15.9 62.5 4 98.9 58 26.4 39.1 77.9 32.5 104.1 26 5.7-23.5 17.9-44.5 34.7-60.9-140.7-25.2-199.4-111.1-199.4-213.3 0-49.5 16.4-95.1 48.4-131.8-20.4-60.6 1.9-112.4 4.9-120.1 58.2-5.2 118.5 41.6 123.3 45.3 33.1-8.9 70.8-13.7 112.9-13.7 42.4 0 80.3 4.9 113.5 13.9 11.3-8.6 67.3-48.8 121.4-43.9 2.9 7.7 24.7 58.3 5.5 118.1 32.5 36.8 49 82.8 49 132.4 0 102.3-59 188.3-200.2 213.2 23.5 23.3 38.1 55.5 38.1 91.1v112.7c0.8 9 0 17.9 15.1 17.9C832.7 877 960.4 709.4 960.4 512.1c0-247.5-200.6-447.9-447.9-447.9C265 64.1 64.6 264.5 64.6 512z"></path>
        </svg>
        

        

        

        

        
      </a>
    
      <a class="soft-size--primary soft-style--box" href="https://weibo.com/u/5561168966" target="_blank" rel="noopener noreferrer">
        

        

        

          
        <svg class="icon icon-weibo" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
          <path d="M385.682429 733.696q11.995429-19.456 6.290286-39.424t-25.746286-28.598857q-19.456-7.972571-41.691429-0.585143t-34.304 26.258286q-12.580571 19.456-7.460571 39.131429t24.576 28.891429 42.569143 1.462857 35.693714-27.136zM439.442429 664.576q4.534857-7.460571 1.974857-15.140571t-10.020571-10.605714q-7.972571-2.852571-16.310857 0.292571t-12.288 10.605714q-9.728 17.700571 7.460571 25.746286 7.972571 2.852571 16.603429-0.292571t12.580571-10.605714zM538.843572 725.723429q-25.746286 58.294857-90.258286 85.723429t-128 6.875429q-61.147429-19.456-84.260571-72.265143t3.730286-107.154286q26.843429-53.174857 86.601143-79.433143t120.32-10.825143q63.414857 16.603429 90.550857 68.315429t1.462857 108.836571zM717.165858 634.294857q-5.12-54.857143-50.834286-97.133714t-119.149714-62.317714-156.891429-11.995429q-127.414857 13.165714-211.163429 80.822857t-75.702857 151.113143q5.12 54.857143 50.834286 97.133714t119.149714 62.317714 156.891429 11.995429q127.414857-13.165714 211.163429-80.822857t75.702857-151.113143zM893.147572 636.562286q0 38.838857-21.138286 79.725714t-62.317714 78.262857-96.256 67.145143-129.170286 47.396571-154.550857 17.700571-157.110857-19.163429-137.435429-53.174857-98.011429-86.308571-37.156571-114.029714q0-65.682286 39.716571-139.995429t112.859429-147.456q96.548571-96.548571 195.145143-134.875429t140.873143 4.022857q37.156571 36.571429 11.410286 119.442286-2.267429 7.972571-0.585143 11.410286t5.705143 4.022857 8.265143-0.292571 7.68-1.974857l3.437714-1.170286q79.433143-33.718857 140.580571-33.718857t87.405714 34.889143q25.746286 35.986286 0 101.741714-1.170286 7.460571-2.56 11.410286t2.56 7.168 6.875429 4.315429 9.728 3.437714q32.548571 10.313143 58.88 26.843429t45.714286 46.592 19.456 66.56zM850.871001 279.990857q23.990857 26.843429 31.158857 62.025143t-3.730286 67.145143q-4.534857 13.165714-16.822857 19.456t-25.453714 2.267429q-13.165714-4.534857-19.456-16.822857t-2.267429-25.453714q11.410286-35.986286-13.677714-63.414857t-61.147429-19.968q-13.677714 2.852571-25.746286-4.534857t-14.262857-21.138286q-2.852571-13.677714 4.534857-25.453714t21.138286-14.555429q34.304-7.460571 68.022857 3.145143t57.709714 37.449143zM954.295001 186.88q49.737143 54.857143 64.292571 127.122286t-7.68 138.020571q-5.12 15.433143-19.456 22.820571t-29.696 2.267429-22.820571-19.456-2.852571-29.696q16.018286-46.884571 5.705143-98.304t-45.714286-90.258286q-35.401143-39.424-84.553143-54.564571t-98.889143-4.827429q-16.018286 3.437714-29.696-5.412571t-17.115429-24.868571 5.412571-29.403429 24.868571-16.822857q70.290286-14.848 139.410286 6.582857t118.857143 76.873143z"></path>
        </svg>
        

        
      </a>
    
      <a class="soft-size--primary soft-style--box" href="https://www.zhihu.com/people/miku-84" target="_blank" rel="noopener noreferrer">
        

        

        

        

          
        <svg class="icon icon-zhihu" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
          <path d="M544.949 561.422s0-71.387-34.779-75.050c-34.779-3.663-142.775 0-142.775 0v-219.654h161.078s-1.83-73.219-32.949-73.219h-261.755l43.93-117.148s-65.897 3.663-89.692 45.761-98.844 252.604-98.844 252.604 25.627 10.983 67.726-20.134c42.101-31.116 56.743-86.033 56.743-86.033l76.879-3.663 1.83 223.316s-133.621-1.83-161.078 0c-27.457 1.83-42.101 75.050-42.101 75.050h203.182s-18.307 124.47-69.557 214.164c-53.085 89.692-151.929 161.078-151.929 161.078s71.387 29.287 140.947-10.983c69.557-42.101 120.811-223.316 120.811-223.316l162.912 203.182s14.643-97.013-1.83-124.47c-18.307-27.457-113.49-137.283-113.49-137.283l-42.101 36.607 29.287-120.811h177.552zM587.050 188.010l-1.83 660.793h65.897l23.795 82.37 115.321-82.37h162.912v-660.793h-366.091zM879.92 775.584h-76.879l-97.013 75.050-21.965-75.050h-20.134v-512.527h215.991v512.527z"></path>
        </svg>
        
      </a>
    
  </div>
  

  <div class="colour-bar"></div>
  
  
  <p>© 2021 <a href="https://github.com/miiiku/hugo-theme-kagome">kagome</a>.</p>
  

  

  <p>
    Powered by
    <a href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a>
    Theme - 
    <a href="https://github.com/miiiku/hugo-theme-kagome" target="_blank" rel="noopener noreferrer author">kagome</a>
  </p>

  <p>
    <a href="javascript:;" id="theme-light">🌞 light</a>
    <a href="javascript:;" id="theme-dark">🌛 dark</a>
    <a href="javascript:;" id="theme-auto">🤖️ auto</a>
  </p>
</footer>


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>






</body>

</html>