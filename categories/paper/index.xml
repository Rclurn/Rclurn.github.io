<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper on Rclurn</title>
    <link>https://Rclurn.github.io/categories/paper/</link>
    <description>Recent content in Paper on Rclurn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>liu-huo@live.com (摸铁校尉🥉)</managingEditor>
    <webMaster>liu-huo@live.com (摸铁校尉🥉)</webMaster>
    <copyright>© 2021 &lt;a href=&#34;https://github.com/miiiku/hugo-theme-kagome&#34;&gt;kagome&lt;/a&gt;.</copyright>
    <lastBuildDate>Wed, 25 Sep 2024 18:07:35 +0800</lastBuildDate><atom:link href="https://Rclurn.github.io/categories/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepSeek V2</title>
      <link>https://Rclurn.github.io/paper/deepseek-v2/</link>
      <pubDate>Wed, 25 Sep 2024 18:07:35 +0800</pubDate>
      <author>liu-huo@live.com (摸铁校尉🥉)</author>
      <guid>https://Rclurn.github.io/paper/deepseek-v2/</guid>
      <description>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model 🚀</description>
    </item>
    
    <item>
      <title>Baichuan 2</title>
      <link>https://Rclurn.github.io/paper/baichuan-2/</link>
      <pubDate>Wed, 25 Sep 2024 18:07:16 +0800</pubDate>
      <author>liu-huo@live.com (摸铁校尉🥉)</author>
      <guid>https://Rclurn.github.io/paper/baichuan-2/</guid>
      <description>Baichuan 2: Open Large-scale Language Models 🚀</description>
    </item>
    
    <item>
      <title>Speculative Decoding</title>
      <link>https://Rclurn.github.io/paper/speculative-decoding/</link>
      <pubDate>Wed, 25 Sep 2024 17:55:09 +0800</pubDate>
      <author>liu-huo@live.com (摸铁校尉🥉)</author>
      <guid>https://Rclurn.github.io/paper/speculative-decoding/</guid>
      <description>Fast Inference from Transformers via Speculative Decoding 🚀</description>
    </item>
    
    <item>
      <title>BAAI General Embeddings</title>
      <link>https://Rclurn.github.io/paper/baai-general-embeddings/</link>
      <pubDate>Tue, 24 Sep 2024 22:03:28 +0800</pubDate>
      <author>liu-huo@live.com (摸铁校尉🥉)</author>
      <guid>https://Rclurn.github.io/paper/baai-general-embeddings/</guid>
      <description>第一阶段：RetroMAE预训练 Navie版 模型架构为encoder-decoder架构，在encoder端，对sentence进行15%～</description>
    </item>
    
    <item>
      <title>Direct Preference Optimization</title>
      <link>https://Rclurn.github.io/paper/direct-preference-optimization/</link>
      <pubDate>Mon, 23 Sep 2024 14:01:16 +0800</pubDate>
      <author>liu-huo@live.com (摸铁校尉🥉)</author>
      <guid>https://Rclurn.github.io/paper/direct-preference-optimization/</guid>
      <description>DPO的优势 相较于PPO，需要的资源更少，不用单独训练Reward模型，也不需要Critic模型； 训练过程中不需要采样 DPO算法 DPO算法的</description>
    </item>
    
    <item>
      <title>Proximal Policy Optimization</title>
      <link>https://Rclurn.github.io/paper/proximal-policy-optimization/</link>
      <pubDate>Mon, 23 Sep 2024 13:57:47 +0800</pubDate>
      <author>liu-huo@live.com (摸铁校尉🥉)</author>
      <guid>https://Rclurn.github.io/paper/proximal-policy-optimization/</guid>
      <description>概述 在PPO中主要存在四个模型：Actor、Ref、Reward和Critic。Actor和Ref来源于同一个模型，一般用SFT后的模型进行</description>
    </item>
    
  </channel>
</rss>
